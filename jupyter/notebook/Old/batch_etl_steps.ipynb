{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.add_jars('/app/postgresql-42.1.4.jar')\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Stocks:ETL\")\n",
    "    .config(\"spark.driver.memory\", \"512m\")\n",
    "    .config(\"spark.driver.cores\", \"1\")\n",
    "    .config(\"spark.executor.memory\", \"512m\")\n",
    "    .config(\"spark.executor.cores\", \"1\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.5'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_dir = '/dataset/stocks-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .csv(stocks_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- OpenInt: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.count()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+------+------+------+------+-------+\n",
      "|               Date|  Open|  High|   Low| Close|Volume|OpenInt|\n",
      "+-------------------+------+------+------+------+------+-------+\n",
      "|1962-01-02 00:00:00| 6.413| 6.413|6.3378|6.3378|467056|      0|\n",
      "|1962-01-03 00:00:00|6.3378|6.3963|6.3378|6.3963|350294|      0|\n",
      "|1962-01-04 00:00:00|6.3963|6.3963|6.3295|6.3295|314365|      0|\n",
      "|1962-01-05 00:00:00|6.3211|6.3211|6.1958|6.2041|440112|      0|\n",
      "|1962-01-08 00:00:00|6.2041|6.2041|6.0373| 6.087|655676|      0|\n",
      "|1962-01-09 00:00:00|6.1208|6.2376|6.1208|6.1621|592806|      0|\n",
      "|1962-01-10 00:00:00|6.1707|6.2041|6.1707|6.1707|359274|      0|\n",
      "|1962-01-11 00:00:00|6.1875|6.2376|6.1875|6.2376|386220|      0|\n",
      "|1962-01-12 00:00:00|6.2543|6.2962|6.2543|6.2543|529933|      0|\n",
      "|1962-01-15 00:00:00|6.2708|6.2962|6.2708|6.2792|305383|      0|\n",
      "|1962-01-16 00:00:00|6.2708|6.2708|6.2128|6.2128|305383|      0|\n",
      "|1962-01-17 00:00:00|6.1875|6.1875|6.0956|6.1125|502984|      0|\n",
      "|1962-01-18 00:00:00|6.1291|6.1875|6.1291|6.1291|449093|      0|\n",
      "|1962-01-19 00:00:00|6.1291|6.1457|6.0624|6.1374|485021|      0|\n",
      "|1962-01-22 00:00:00|6.1374|6.1958|6.1208|6.1208|332329|      0|\n",
      "|1962-01-23 00:00:00|6.1208|6.1291|6.0538|6.0624|449093|      0|\n",
      "|1962-01-24 00:00:00|6.0624|6.0956|6.0287|6.0956|494001|      0|\n",
      "|1962-01-25 00:00:00|6.0956|6.1457|6.0208|6.0287|386220|      0|\n",
      "|1962-01-26 00:00:00|6.0287|6.0538|5.9951|5.9951|296401|      0|\n",
      "|1962-01-29 00:00:00|5.9951|6.0373|5.8952|5.8952|700585|      0|\n",
      "+-------------------+------+------+------+------+------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('filename', F.input_file_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+------+------+------+------+-------+---------------------------------------+\n",
      "|Date               |Open  |High  |Low   |Close |Volume|OpenInt|filename                               |\n",
      "+-------------------+------+------+------+------+------+-------+---------------------------------------+\n",
      "|1962-01-02 00:00:00|6.413 |6.413 |6.3378|6.3378|467056|0      |file:///dataset/stocks-small/ibm.us.txt|\n",
      "|1962-01-03 00:00:00|6.3378|6.3963|6.3378|6.3963|350294|0      |file:///dataset/stocks-small/ibm.us.txt|\n",
      "|1962-01-04 00:00:00|6.3963|6.3963|6.3295|6.3295|314365|0      |file:///dataset/stocks-small/ibm.us.txt|\n",
      "|1962-01-05 00:00:00|6.3211|6.3211|6.1958|6.2041|440112|0      |file:///dataset/stocks-small/ibm.us.txt|\n",
      "|1962-01-08 00:00:00|6.2041|6.2041|6.0373|6.087 |655676|0      |file:///dataset/stocks-small/ibm.us.txt|\n",
      "|1962-01-09 00:00:00|6.1208|6.2376|6.1208|6.1621|592806|0      |file:///dataset/stocks-small/ibm.us.txt|\n",
      "|1962-01-10 00:00:00|6.1707|6.2041|6.1707|6.1707|359274|0      |file:///dataset/stocks-small/ibm.us.txt|\n",
      "|1962-01-11 00:00:00|6.1875|6.2376|6.1875|6.2376|386220|0      |file:///dataset/stocks-small/ibm.us.txt|\n",
      "|1962-01-12 00:00:00|6.2543|6.2962|6.2543|6.2543|529933|0      |file:///dataset/stocks-small/ibm.us.txt|\n",
      "|1962-01-15 00:00:00|6.2708|6.2962|6.2708|6.2792|305383|0      |file:///dataset/stocks-small/ibm.us.txt|\n",
      "|1962-01-16 00:00:00|6.2708|6.2708|6.2128|6.2128|305383|0      |file:///dataset/stocks-small/ibm.us.txt|\n",
      "|1962-01-17 00:00:00|6.1875|6.1875|6.0956|6.1125|502984|0      |file:///dataset/stocks-small/ibm.us.txt|\n",
      "|1962-01-18 00:00:00|6.1291|6.1875|6.1291|6.1291|449093|0      |file:///dataset/stocks-small/ibm.us.txt|\n",
      "|1962-01-19 00:00:00|6.1291|6.1457|6.0624|6.1374|485021|0      |file:///dataset/stocks-small/ibm.us.txt|\n",
      "|1962-01-22 00:00:00|6.1374|6.1958|6.1208|6.1208|332329|0      |file:///dataset/stocks-small/ibm.us.txt|\n",
      "|1962-01-23 00:00:00|6.1208|6.1291|6.0538|6.0624|449093|0      |file:///dataset/stocks-small/ibm.us.txt|\n",
      "|1962-01-24 00:00:00|6.0624|6.0956|6.0287|6.0956|494001|0      |file:///dataset/stocks-small/ibm.us.txt|\n",
      "|1962-01-25 00:00:00|6.0956|6.1457|6.0208|6.0287|386220|0      |file:///dataset/stocks-small/ibm.us.txt|\n",
      "|1962-01-26 00:00:00|6.0287|6.0538|5.9951|5.9951|296401|0      |file:///dataset/stocks-small/ibm.us.txt|\n",
      "|1962-01-29 00:00:00|5.9951|6.0373|5.8952|5.8952|700585|0      |file:///dataset/stocks-small/ibm.us.txt|\n",
      "+-------------------+------+------+------+------+------+-------+---------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lookup = spark.read.csv('/dataset/yahoo-symbols-201709.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------+--------------------+-------+\n",
      "|   _c0|                 _c1|     _c2|                 _c3|    _c4|\n",
      "+------+--------------------+--------+--------------------+-------+\n",
      "|Ticker|                Name|Exchange|       Category Name|Country|\n",
      "|  OEDV|Osage Exploration...|     PNK|                null|    USA|\n",
      "|  AAPL|          Apple Inc.|     NMS|Electronic Equipment|    USA|\n",
      "|   BAC|Bank of America C...|     NYQ|  Money Center Banks|    USA|\n",
      "|  AMZN|    Amazon.com, Inc.|     NMS|Catalog & Mail Or...|    USA|\n",
      "|     T|           AT&T Inc.|     NYQ|Telecom Services ...|    USA|\n",
      "|  GOOG|       Alphabet Inc.|     NMS|Internet Informat...|    USA|\n",
      "|    MO|  Altria Group, Inc.|     NYQ|          Cigarettes|    USA|\n",
      "|   DAL|Delta Air Lines, ...|     NYQ|      Major Airlines|    USA|\n",
      "|    AA|   Alcoa Corporation|     NYQ|            Aluminum|    USA|\n",
      "|   AXP|American Express ...|     NYQ|     Credit Services|    USA|\n",
      "|    DD|E. I. du Pont de ...|     NYQ|Agricultural Chem...|    USA|\n",
      "|  BABA|Alibaba Group Hol...|     NYQ|Specialty Retail,...|    USA|\n",
      "|   ABT| Abbott Laboratories|     NYQ|Medical Appliance...|    USA|\n",
      "|    UA|  Under Armour, Inc.|     NYQ|Textile - Apparel...|    USA|\n",
      "|  AMAT|Applied Materials...|     NMS|Semiconductor Equ...|    USA|\n",
      "|  AMGN|          Amgen Inc.|     NMS|       Biotechnology|    USA|\n",
      "|   AAL|American Airlines...|     NMS|      Major Airlines|    USA|\n",
      "|   AIG|American Internat...|     NYQ|Property & Casual...|    USA|\n",
      "|   ALL|The Allstate Corp...|     NYQ|Property & Casual...|    USA|\n",
      "+------+--------------------+--------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_lookup.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_symbol_from(filename):\n",
    "    return filename.split('/')[-1].split('.')[0].upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IBM'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filename = 'file:///dataset/stocks-small/ibm.us.txt' # => IBM\n",
    "extract_symbol_from('file:///dataset/stocks-small/ibm.us.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_symbol = F.udf(lambda filename: extract_symbol_from(filename), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_folder = stocks_dir\n",
    "df = spark.read \\\n",
    "        .option(\"header\", True) \\\n",
    "        .option(\"inferSchema\", True) \\\n",
    "        .csv(stocks_folder) \\\n",
    "        .withColumn(\"name\", extract_symbol(F.input_file_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+------+------+------+------+-------+----+\n",
      "|               Date|  Open|  High|   Low| Close|Volume|OpenInt|name|\n",
      "+-------------------+------+------+------+------+------+-------+----+\n",
      "|1962-01-02 00:00:00| 6.413| 6.413|6.3378|6.3378|467056|      0| IBM|\n",
      "|1962-01-03 00:00:00|6.3378|6.3963|6.3378|6.3963|350294|      0| IBM|\n",
      "|1962-01-04 00:00:00|6.3963|6.3963|6.3295|6.3295|314365|      0| IBM|\n",
      "|1962-01-05 00:00:00|6.3211|6.3211|6.1958|6.2041|440112|      0| IBM|\n",
      "|1962-01-08 00:00:00|6.2041|6.2041|6.0373| 6.087|655676|      0| IBM|\n",
      "+-------------------+------+------+------+------+------+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "        .option(\"header\", True) \\\n",
    "        .option(\"inferSchema\", True) \\\n",
    "        .csv(stocks_folder) \\\n",
    "        .withColumn(\"name\", extract_symbol(F.input_file_name())) \\\n",
    "        .withColumnRenamed(\"Date\", \"dateTime\") \\\n",
    "        .withColumnRenamed(\"Open\", \"open\") \\\n",
    "        .withColumnRenamed(\"High\", \"high\") \\\n",
    "        .withColumnRenamed(\"Low\", \"low\") \\\n",
    "        .withColumnRenamed(\"Close\", \"close\") \\\n",
    "        .drop(\"Volume\", \"OpenInt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stocks = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+------+------+------+----+\n",
      "|           dateTime|  open|  high|   low| close|name|\n",
      "+-------------------+------+------+------+------+----+\n",
      "|1962-01-02 00:00:00| 6.413| 6.413|6.3378|6.3378| IBM|\n",
      "|1962-01-03 00:00:00|6.3378|6.3963|6.3378|6.3963| IBM|\n",
      "|1962-01-04 00:00:00|6.3963|6.3963|6.3295|6.3295| IBM|\n",
      "|1962-01-05 00:00:00|6.3211|6.3211|6.1958|6.2041| IBM|\n",
      "|1962-01-08 00:00:00|6.2041|6.2041|6.0373| 6.087| IBM|\n",
      "+-------------------+------+------+------+------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stocks.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_file = '/dataset/yahoo-symbols-201709.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols_lookup = spark.read. \\\n",
    "        option(\"header\", True). \\\n",
    "        option(\"inferSchema\", True). \\\n",
    "        csv(lookup_file). \\\n",
    "        select(\"Ticker\", \"Category Name\"). \\\n",
    "        withColumnRenamed(\"Ticker\", \"symbol\"). \\\n",
    "        withColumnRenamed(\"Category Name\", \"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+------+------+------+----+\n",
      "|           dateTime|  open|  high|   low| close|name|\n",
      "+-------------------+------+------+------+------+----+\n",
      "|1962-01-02 00:00:00| 6.413| 6.413|6.3378|6.3378| IBM|\n",
      "|1962-01-03 00:00:00|6.3378|6.3963|6.3378|6.3963| IBM|\n",
      "|1962-01-04 00:00:00|6.3963|6.3963|6.3295|6.3295| IBM|\n",
      "+-------------------+------+------+------+------+----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------+--------------------+\n",
      "|symbol|            category|\n",
      "+------+--------------------+\n",
      "|  OEDV|                null|\n",
      "|  AAPL|Electronic Equipment|\n",
      "|   BAC|  Money Center Banks|\n",
      "+------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_stocks.show(3)\n",
    "symbols_lookup.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = df_stocks \\\n",
    "    .withColumnRenamed('dateTime', \"full_date\") \\\n",
    "    .filter(\"full_date >= \\\"2017-09-01\\\"\") \\\n",
    "    .withColumn(\"year\", F.year(\"full_date\")) \\\n",
    "    .withColumn(\"month\", F.month(\"full_date\")) \\\n",
    "    .withColumn(\"day\", F.dayofmonth(\"full_date\")) \\\n",
    "    .withColumnRenamed(\"name\", \"symbol\") \\\n",
    "    .join(symbols_lookup, [\"symbol\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+------+------+------+------+----+-----+---+--------------------+\n",
      "|symbol|          full_date|  open|  high|   low| close|year|month|day|            category|\n",
      "+------+-------------------+------+------+------+------+----+-----+---+--------------------+\n",
      "|   IBM|2017-09-01 00:00:00|141.57|143.07|141.57|142.65|2017|    9|  1|Information Techn...|\n",
      "|   IBM|2017-09-05 00:00:00|142.08|142.93|141.29|141.62|2017|    9|  5|Information Techn...|\n",
      "|   IBM|2017-09-06 00:00:00|142.46|143.04|142.08| 142.4|2017|    9|  6|Information Techn...|\n",
      "+------+-------------------+------+------+------+------+----+-----+---+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "window20 = (Window.partitionBy(F.col('symbol')).orderBy(F.col(\"full_date\")).rowsBetween(-20, 0))\n",
    "window50 = (Window.partitionBy(F.col('symbol')).orderBy(F.col(\"full_date\")).rowsBetween(-50, 0))\n",
    "window100 = (Window.partitionBy(F.col('symbol')).orderBy(F.col(\"full_date\")).rowsBetween(-100, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_moving_avg_df = joined_df \\\n",
    "    .withColumn(\"ma20\", F.avg(\"close\").over(window20)) \\\n",
    "    .withColumn(\"ma50\", F.avg(\"close\").over(window50)) \\\n",
    "    .withColumn(\"ma100\", F.avg(\"close\").over(window100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------------------+\n",
      "|symbol| close|              ma20|\n",
      "+------+------+------------------+\n",
      "|  AAPL|163.46|            163.46|\n",
      "|  AAPL| 161.5|162.48000000000002|\n",
      "|  AAPL|161.33| 162.0966666666667|\n",
      "|  AAPL|160.68|          161.7425|\n",
      "|  AAPL|158.06|           161.006|\n",
      "|  AAPL|160.92|160.99166666666665|\n",
      "|  AAPL|160.28|160.89000000000001|\n",
      "|  AAPL|159.08|         160.66375|\n",
      "|  AAPL|157.71|160.33555555555554|\n",
      "|  AAPL|159.31|           160.233|\n",
      "|  AAPL| 158.1| 160.0390909090909|\n",
      "|  AAPL|158.16|          159.8825|\n",
      "|  AAPL|155.51|159.54615384615383|\n",
      "|  AAPL|152.84|159.06714285714287|\n",
      "|  AAPL|151.35|158.55266666666665|\n",
      "|  AAPL|150.01|         158.01875|\n",
      "|  AAPL|152.59| 157.6994117647059|\n",
      "|  AAPL|153.68|157.47611111111112|\n",
      "|  AAPL|152.73| 157.2263157894737|\n",
      "|  AAPL|153.57|157.04350000000002|\n",
      "|  AAPL|153.26|156.86333333333334|\n",
      "|  AAPL|153.92| 156.4090476190476|\n",
      "|  AAPL|152.93|156.00095238095238|\n",
      "|  AAPL|154.83|155.69142857142856|\n",
      "|  AAPL|154.74|155.40857142857143|\n",
      "+------+------+------------------+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Moving Average\n",
    "stocks_moving_avg_df.select('symbol', 'close', 'ma20').show(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/dataset/output.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_moving_avg_df \\\n",
    "    .write \\\n",
    "    .mode('overwrite') \\\n",
    "    .partitionBy(\"year\", \"month\", \"day\") \\\n",
    "    .parquet(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet = spark.read.parquet(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "950"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_parquet.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parquet.createOrReplaceTempView(\"stocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[symbol#559], functions=[max(close#564)])\n",
      "+- Exchange hashpartitioning(symbol#559, 2)\n",
      "   +- *(1) HashAggregate(keys=[symbol#559], functions=[partial_max(close#564)])\n",
      "      +- *(1) Project [symbol#559, close#564]\n",
      "         +- *(1) Filter ((isnotnull(full_date#560) && (cast(full_date#560 as string) >= 2017-09-01)) && (cast(full_date#560 as string) < 2017-10-01))\n",
      "            +- *(1) FileScan parquet [symbol#559,full_date#560,close#564,year#569,month#570,day#571] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/dataset/output.parquet], PartitionCount: 50, PartitionFilters: [], PushedFilters: [IsNotNull(full_date)], ReadSchema: struct<symbol:string,full_date:timestamp,close:double>\n"
     ]
    }
   ],
   "source": [
    "badHighestClosingPrice = spark.sql(\"SELECT symbol, MAX(close) AS price FROM stocks WHERE full_date >= '2017-09-01' AND full_date < '2017-10-01' GROUP BY symbol\")\n",
    "badHighestClosingPrice.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[symbol#559], functions=[max(close#564)])\n",
      "+- Exchange hashpartitioning(symbol#559, 2)\n",
      "   +- *(1) HashAggregate(keys=[symbol#559], functions=[partial_max(close#564)])\n",
      "      +- *(1) Project [symbol#559, close#564]\n",
      "         +- *(1) FileScan parquet [symbol#559,close#564,year#569,month#570,day#571] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/dataset/output.parquet], PartitionCount: 20, PartitionFilters: [isnotnull(year#569), isnotnull(month#570), (year#569 = 2017), (month#570 = 9)], PushedFilters: [], ReadSchema: struct<symbol:string,close:double>\n"
     ]
    }
   ],
   "source": [
    "highestClosingPrice = spark.sql(\"SELECT symbol, MAX(close) AS price FROM stocks WHERE year=2017 AND month=9 GROUP BY symbol\")\n",
    "highestClosingPrice.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o219.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 34.0 failed 4 times, most recent failure: Lost task 0.3 in stage 34.0 (TID 160, 172.19.0.12, executor 1): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO workshop.stocks (\"symbol\",\"full_date\",\"open\",\"high\",\"low\",\"close\",\"category\",\"ma20\",\"ma50\",\"ma100\") VALUES ('AAPL','2017-09-01 00:00:00+00'::timestamp with time zone,164.21,164.35,163.04,163.46,'Electronic Equipment',163.46,163.46,163.46) was aborted: ERROR: duplicate key value violates unique constraint \"stocks_pkey\"\n  Detail: Key (full_date, symbol)=(2017-09-01 00:00:00+00, AAPL) already exists.  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:148)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2191)\n\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1325)\n\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1350)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:458)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:791)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1563)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:676)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:838)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:838)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:980)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:980)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"stocks_pkey\"\n  Detail: Key (full_date, symbol)=(2017-09-01 00:00:00+00, AAPL) already exists.\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2477)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2190)\n\t... 20 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:980)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:978)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:978)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:838)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:68)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO workshop.stocks (\"symbol\",\"full_date\",\"open\",\"high\",\"low\",\"close\",\"category\",\"ma20\",\"ma50\",\"ma100\") VALUES ('AAPL','2017-09-01 00:00:00+00'::timestamp with time zone,164.21,164.35,163.04,163.46,'Electronic Equipment',163.46,163.46,163.46) was aborted: ERROR: duplicate key value violates unique constraint \"stocks_pkey\"\n  Detail: Key (full_date, symbol)=(2017-09-01 00:00:00+00, AAPL) already exists.  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:148)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2191)\n\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1325)\n\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1350)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:458)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:791)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1563)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:676)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:838)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:838)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:980)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:980)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"stocks_pkey\"\n  Detail: Key (full_date, symbol)=(2017-09-01 00:00:00+00, AAPL) already exists.\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2477)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2190)\n\t... 20 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-d9a2c17af6f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"password\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w0rkzh0p\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"driver\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"org.postgresql.Driver\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'append'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o219.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 34.0 failed 4 times, most recent failure: Lost task 0.3 in stage 34.0 (TID 160, 172.19.0.12, executor 1): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO workshop.stocks (\"symbol\",\"full_date\",\"open\",\"high\",\"low\",\"close\",\"category\",\"ma20\",\"ma50\",\"ma100\") VALUES ('AAPL','2017-09-01 00:00:00+00'::timestamp with time zone,164.21,164.35,163.04,163.46,'Electronic Equipment',163.46,163.46,163.46) was aborted: ERROR: duplicate key value violates unique constraint \"stocks_pkey\"\n  Detail: Key (full_date, symbol)=(2017-09-01 00:00:00+00, AAPL) already exists.  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:148)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2191)\n\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1325)\n\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1350)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:458)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:791)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1563)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:676)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:838)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:838)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:980)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:980)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"stocks_pkey\"\n  Detail: Key (full_date, symbol)=(2017-09-01 00:00:00+00, AAPL) already exists.\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2477)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2190)\n\t... 20 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:980)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:978)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:978)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:838)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:68)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO workshop.stocks (\"symbol\",\"full_date\",\"open\",\"high\",\"low\",\"close\",\"category\",\"ma20\",\"ma50\",\"ma100\") VALUES ('AAPL','2017-09-01 00:00:00+00'::timestamp with time zone,164.21,164.35,163.04,163.46,'Electronic Equipment',163.46,163.46,163.46) was aborted: ERROR: duplicate key value violates unique constraint \"stocks_pkey\"\n  Detail: Key (full_date, symbol)=(2017-09-01 00:00:00+00, AAPL) already exists.  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:148)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2191)\n\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1325)\n\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1350)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:458)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:791)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1563)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:676)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:838)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$saveTable$1.apply(JdbcUtils.scala:838)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:980)\n\tat org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:980)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"stocks_pkey\"\n  Detail: Key (full_date, symbol)=(2017-09-01 00:00:00+00, AAPL) already exists.\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2477)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2190)\n\t... 20 more\n"
     ]
    }
   ],
   "source": [
    "# Write to Postgres\n",
    "stocks_moving_avg_df \\\n",
    "    .drop(\"year\", \"month\", \"day\") \\\n",
    "    .write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://postgres/workshop\") \\\n",
    "    .option(\"dbtable\", \"workshop.stocks\") \\\n",
    "    .option(\"user\", \"workshop\") \\\n",
    "    .option(\"password\", \"w0rkzh0p\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .mode('append') \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "consult = spark.sql(\"SELECT * FROM stocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+-------+-------+-------+------+--------------------+------------------+------------------+------------------+----+-----+---+\n",
      "|symbol|          full_date|   open|   high|    low| close|            category|              ma20|              ma50|             ma100|year|month|day|\n",
      "+------+-------------------+-------+-------+-------+------+--------------------+------------------+------------------+------------------+----+-----+---+\n",
      "|  AAPL|2017-09-08 00:00:00| 160.28| 160.57| 157.96|158.06|Electronic Equipment|           161.006|           161.006|           161.006|2017|    9|  8|\n",
      "|  BABA|2017-09-08 00:00:00| 169.99| 171.22| 168.92| 169.0|Specialty Retail,...|169.71599999999998|169.71599999999998|169.71599999999998|2017|    9|  8|\n",
      "|  EBAY|2017-09-08 00:00:00|  38.04|   38.3| 37.725| 37.79|Specialty Retail,...| 36.99999999999999| 36.99999999999999| 36.99999999999999|2017|    9|  8|\n",
      "|    FB|2017-09-08 00:00:00| 173.09| 173.49|  170.8|170.95|Internet Informat...|           171.798|           171.798|           171.798|2017|    9|  8|\n",
      "|  GOOG|2017-09-08 00:00:00| 936.49| 936.99| 924.88| 926.5|Internet Informat...| 931.2239999999999| 931.2239999999999| 931.2239999999999|2017|    9|  8|\n",
      "| GOOGL|2017-09-08 00:00:00|  949.7|  950.7| 940.01|940.95|Internet Informat...|           945.266|           945.266|           945.266|2017|    9|  8|\n",
      "|   IBM|2017-09-08 00:00:00| 141.73| 141.73| 140.24|141.04|Information Techn...|141.83999999999997|141.83999999999997|141.83999999999997|2017|    9|  8|\n",
      "|   JNJ|2017-09-08 00:00:00|  132.0| 132.34| 130.87|130.98|Drug Manufacturer...|           130.954|           130.954|           130.954|2017|    9|  8|\n",
      "|  ORCL|2017-09-08 00:00:00| 51.596| 51.676| 51.178|51.377|Application Software|            50.973|            50.973|            50.973|2017|    9|  8|\n",
      "|   WDC|2017-09-08 00:00:00| 87.902| 88.325| 85.774| 86.51|Data Storage Devices|           88.4766|           88.4766|           88.4766|2017|    9|  8|\n",
      "|   XRX|2017-09-08 00:00:00| 31.516| 31.665| 31.308|31.328|Information Techn...|           31.8178|           31.8178|           31.8178|2017|    9|  8|\n",
      "|  AAPL|2017-11-01 00:00:00| 169.26| 169.33| 165.02|166.29|Electronic Equipment|158.03095238095236|157.36511627906975|157.36511627906975|2017|   11|  1|\n",
      "|  BABA|2017-11-01 00:00:00| 187.88| 188.88| 183.58|186.08|Specialty Retail,...| 178.6428571428571|176.24348837209297|176.24348837209297|2017|   11|  1|\n",
      "|  EBAY|2017-11-01 00:00:00|  37.85|  38.09| 37.435| 37.54|Specialty Retail,...| 37.77619047619047| 37.88186046511627| 37.88186046511627|2017|   11|  1|\n",
      "|    FB|2017-11-01 00:00:00| 182.36|  182.9| 180.57|182.66|Internet Informat...|174.09428571428572|172.26093023255814|172.26093023255814|2017|   11|  1|\n",
      "|  GOOG|2017-11-01 00:00:00|1017.21|1029.67|1016.95|1025.5|Internet Informat...| 987.1190476190476| 959.6888372093022| 959.6888372093022|2017|   11|  1|\n",
      "| GOOGL|2017-11-01 00:00:00|1036.32|1047.86| 1034.0|1042.6|Internet Informat...|1004.0114285714286| 975.3641860465116| 975.3641860465116|2017|   11|  1|\n",
      "|   IBM|2017-11-01 00:00:00| 152.57| 153.37| 152.31|152.51|Information Techn...| 150.4890476190476|146.96395348837208|146.96395348837208|2017|   11|  1|\n",
      "|   JNJ|2017-11-01 00:00:00| 139.83| 140.59| 139.34|139.98|Drug Manufacturer...|138.53285714285715|135.17441860465112|135.17441860465112|2017|   11|  1|\n",
      "|  ORCL|2017-11-01 00:00:00|   51.1|  51.17|  50.57| 50.64|Application Software| 49.31457142857144| 49.42353488372093| 49.42353488372093|2017|   11|  1|\n",
      "+------+-------------------+-------+-------+-------+------+--------------------+------------------+------------------+------------------+----+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "consult.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://postgres/workshop\") \\\n",
    "    .option(\"dbtable\", \"workshop.stocks\") \\\n",
    "    .option(\"user\", \"workshop\") \\\n",
    "    .option(\"password\", \"w0rkzh0p\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
